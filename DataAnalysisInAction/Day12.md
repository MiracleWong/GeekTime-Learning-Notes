# 【第三期】21 天打卡行动

#21 天打卡# Day12
专栏：数据分析实战 45 讲
时间：2020-02-24
学习：22
学习要点和总结：

一、22 丨 SVM（上）：如何用一根棍子将蓝红两色球分开？

1. SVM 的英文叫 Support Vector Machine，中文名为支持向量机。在机器学习中，SVM 是有监督的学习模型。SVM 作为有监督的学习模型，通常可以帮我们模式识别、分类以及回归分析。
2. 在非线性 SVM 中，核函数的选择就是影响 SVM 最大的变量。最常用的核函数有线性核、多项式核、高斯核、拉普拉斯核、sigmoid 核。
3. 分类间隔：两个极限位置 A 和 B 之间的分界线 C 就是最优决策面。极限位置到最优决策面 C 之间的距离，就是“分类间隔”，英文叫做 margin。那个拥有“最大间隔”（max margin）的决策面就是 SVM 要找的最优解。
4. 硬间隔指的就是完全分类准确，不能存在分类错误的情况。软间隔，就是允许一定量的样本分类错误。
5. 核函数。它可以将样本从原始空间映射到一个更高维的特质空间中，使得样本在新的空间中线性可分。
6. 在非线性 SVM 中，核函数的选择就是影响 SVM 最大的变量。最常用的核函数有线性核、多项式核、高斯核、拉普拉斯核、sigmoid 核。
7. SVM 本身是一个二值分类器，最初是为二分类问题设计的，也就是回答 Yes 或者是 No。而实际上我们要解决的问题，可能是多分类的情况，比如对文本进行分类，或者对图像进行识别。针对这种情况，我们可以将多个二分类器组合起来形成一个多分类器，常见的方法有“一对多法”和“一对一法”两种。
8. 一对多法：优点：针对 K 个分类，需要训练 K 个分类器，分类速度较快；缺点：训练速度较慢，因为每个分类器都需要对全部样本进行训练，而且负样本数量远大于正样本数量，会造成样本不对称的情况，而且当增加新的分类，比如第 K+1 类时，需要重新对分类器进行构造。
9. 一对一法：优点：如果新增一类，不需要重新训练所有的 SVM，只需要训练和新增这一类样本的分类器。而且这种方式在训练单个 SVM 模型的时候，训练速度快。缺点：分类器的个数与 K 的平方成正比，所以当 K 较大时，训练和测试的时间会比较慢。
