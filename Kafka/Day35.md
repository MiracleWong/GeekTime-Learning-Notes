# 21 天打卡行动

#21 天打卡# Day35
专栏：kafka 核心技术与实战
时间：2020-03-30
学习：16
学习要点和总结：

一、[16 | 揭开神秘的"位移主题"面纱](https://time.geekbang.org/column/article/105473?utm_term=pc_interstitial_208)

1，诞生背景
A ：老版本的 Kafka 会把位移信息保存在 Zk 中，当 Consumer 重启后，自动从 Zk 中读取位移信息。这种设计使 Kafka Broker 不需要保存位移数据，可减少 Broker 端需要持有的状态空间，有利于实现高伸缩性。
B ：但 zk 不适用于高频的写操作，这令 zk 集群性能严重下降，在新版本中将消费者的位移数据作为一条条普通的 Kafka 消息，提交至内部主题（\_consumer_offsets）中保存。实现高持久性和高频写操作。

2，特点:
A ：位移主题是一个普通主题，同样可以被手动创建，修改，删除。。
B ：位移主题的消息格式是 kafka 定义的，不可以被手动修改，若修改格式不正确，kafka 将会崩溃。
C ：位移主题保存了三部分内容：Group ID，主题名，分区号。

3，创建：
A ：当 Kafka 集群中的第一个 Consumer 程序启动时，Kafka 会自动创建位移主题。也可以手动创建
B ：分区数依赖于 Broker 端的 offsets.topic.num.partitions 的取值，默认为 50
C ：副本数依赖于 Broker 端的 offsets.topic.replication.factor 的取值，默认为 3

4，使用：
A ：当 Kafka 提交位移消息时会使用这个主题
B ：位移提交得分方式有两种:手动和自动提交位移。
C ：推荐使用手动提交位移，自动提交位移会存在问题：只有 consumer 一直启动设置，他就会无限期地向主题写入消息。

5，清理：
A ：Kafka 使用 Compact 策略来删除位移主题中的过期消息，避免位移主题无限膨胀。
B ：kafka 提供专门的后台线程定期巡检待 compcat 的主题，查看是否存在满足条件的可删除数据。

6，注意事项：
A ：建议不要修改默认分区数，在 kafka 中有些许功能写死的是 50 个分区
B ：建议不要使用自动提交模式，采用手动提交，避免消费者无限制的写入消息。
C ：后台定期巡检线程叫 Log Cleaner，若线上遇到位移主题无限膨胀占用过多磁盘，应该检查此线程的工作状态。
